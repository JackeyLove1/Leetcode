# 长短时记忆网络
import torch
import torch.nn as nn
batch_size, num_steps = 32, 35

def get_lstm_params(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        return torch.randn(size=shape, device=device)*0.01

    def get_parameters():
        return (normal((num_inputs, num_hiddens)),
                normal((num_hiddens, num_hiddens)),
                torch.zeros(num_hiddens, device=device))

    W_xi, W_hi, b_i = get_parameters()
    W_xf, W_hf, b_f = get_parameters()
    W_xo, W_ho, b_o = get_parameters()
    W_xc, W_hc, b_c = get_parameters()
    
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)
    
    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,
              b_c, W_hq, b_q]
    for param in params:
        param.requires_grad_(True)
    return params
    
    def init_lstm_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device),
            torch.zeros((batch_size, num_hiddens), device=device))
            
   def LSTM(inputs, state, params):
    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,
     W_hq, b_q] = params
    (H, C) = state
    outputs = []
    for X in inputs:
        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)
        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)
        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)
        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)
        C = F * C + I * C_tilda
        H = O * torch.tanh(C)
        Y = (H @ W_hq) + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H, C)


# 载入人体行为识别数据集
import numpy as np

def load_X(X_signals_paths):
    X_signals = []

    for signal_type_path in X_signals_paths:
        file = open(signal_type_path, 'r')
        X_signals.append(
            [np.array(serie, dtype=np.float32) for serie in [
                row.replace('  ', ' ').strip().split(' ') for row in file
            ]]
        )
        file.close()

    return np.transpose(np.array(X_signals), (1, 2, 0))

def load_y(y_path):
    file = open(y_path, 'r')
    y_ = np.array(
        [elem for elem in [
            row.replace('  ', ' ').strip().split(' ') for row in file
        ]],
        dtype=np.int32
    )
    file.close()
    return y_ - 1

# 人体行为识别主干神经网络
import torch
from torch import nn
import torch.nn.functional as F
from torch.autograd import Variable

class LSTMModel(nn.Module):
    def __init__(self, n_input=n_input, n_hidden=n_hidden, n_layers=n_layers,
                 n_classes=n_classes, drop_prob=drop_prob):
        super(LSTMModel, self).__init__()

        self.n_layers = n_layers
        self.n_hidden = n_hidden
        self.n_classes = n_classes
        self.drop_prob = drop_prob
        self.n_input = n_input

        self.lstm1 = LSTM(n_input, n_hidden, n_layers, dropout=self.drop_prob)
        self.lstm2 = LSTM(n_hidden, n_hidden, n_layers, dropout=self.drop_prob)
        self.fc = nn.Linear(n_hidden, n_classes)
        self.dropout = nn.Dropout(drop_prob)

    def forward(self, x, hidden):
        x = x.permute(1, 0, 2)
        x, hidden1 = self.lstm1(x, hidden)
        for i in range(n_highway_layers):
            #x = F.relu(x)
            x, hidden2 = self.lstm2(x, hidden)
        x = self.dropout(x)
        out = x[-1]
        out = out.contiguous().view(-1, self.n_hidden)
        out = self.fc(out)
        out = F.softmax(out)

        return out

    def init_hidden(self, batch_size):
        weight = next(self.parameters()).data
        if (torch.cuda.is_available() ):
            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),
                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())
        else:
            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),
                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())

        return hidden

# 定点量化代码
def signed_quantize(x, bits, bias=None):
    min_val, max_val = x.min(), x.max()
    n = 2.0 ** (bits -1)
    scale = max(abs(min_val), abs(max_val)) / n
    qx = torch.floor(x / scale)
    if bias is not None:
        qb = torch.floor(bias / scale)
        return qx, qb
    else:
        return qx

# 对模型整体进行量化
def scale_quant_model(model, bits):
    net = deepcopy(model)
    params_quant = OrderedDict()
    params_save = OrderedDict()

    for k, v in model.state_dict().items():
        if 'classifier' not in k and 'num_batches' not in k and 'running' not in k:
            if 'weight' in k:
                weight = v
                bias_name = k.replace('weight', 'bias')
                try:
                    bias = model.state_dict()[bias_name]
                    w, b = signed_quantize(weight, bits, bias)
                    params_quant[k] = w
                    params_quant[bias_name] = b
                    if bits > 8 and bits <= 16:
                        params_save[k] = w.short()
                        params_save[bias_name] = b.short()
                    elif bits >1 and bits <= 8:
                        params_save[k] = w.char()
                        params_save[bias_name] = b.char()
                    elif bits == 1:
                        params_save[k] = w.bool()
                        params_save[bias_name] = b.bool()

                except:
                    w = signed_quantize(w, bits)
                    params_quant[k] = w
                    params_save[k] = w.char()

        else:
            params_quant[k] = v
            params_save[k] = v
    net.load_state_dict(params_quant)
    return net, params_save
    
   
# 根据参数和原始浮点数量化为定点数
def Quant(Vx, Q, RQM):
    return round(Q * Vx) - RQM
# 根据量化参数还原回浮点
def QuantRevert(VxQuant, Q, RQM):
    return (VxQuant + RQM) / Q
 
# 针对数组的量化和还原
def ListQuant(data_list, quant_bits):
    # 数组范围估计
    data_min = min(data_list)
    data_max = max(data_list)

    # 量化参数估计
    Q = ((1 << quant_bits) - 1) * 1.0 / (data_max - data_min)
    RQM = (int)(round(Q*data_min))

    # 产生量化后的数组
    quant_data_list = []
    for x in data_list:
        quant_data = Quant(x, Q, RQM)
        quant_data_list.append(quant_data)
    quant_data_list = np.array(quant_data_list)
    return (Q, RQM, quant_data_list)
def ListQuantRevert(quant_data_list, Q, RQM):
    quant_revert_data_list = []
    for quant_data in quant_data_list:
        # 量化数据还原为原始浮点数据
        revert_quant_data = QuantRevert(quant_data, Q, RQM)
        quant_revert_data_list.append(revert_quant_data)
    quant_revert_data_list = np.array(quant_revert_data_list)
    return quant_revert_data_list

// C++移位量化
#define MAX_THREAD 10

void stub(
    vector<vector<int32_t>>& input,
    vector<vector<int8_t>>& shift,
    vector<vector<int32_t>>& sign ,
    vector<int32_t>& bias,
    unsigned int start, 
    unsigned int end,
    unsigned int idx,
    vector<vector<int32_t>>& output)
{
    
    for(unsigned int  batch = 0 ;  batch < input.size(); batch++    ){
        for(unsigned int output_feature = start ; output_feature < end; output_feature++){
            for(unsigned int input_feature = 0; input_feature <input[0].size();input_feature++){
                auto s = shift[output_feature][input_feature];
                auto y = output[batch][output_feature];
                auto x = input[batch][input_feature];
                if(sign[output_feature][input_feature] < 0){
                    y -= (x << s);
                }
                else if(sign[output_feature][input_feature] > 0) {
                    y += (x << s);
                }
                else  {
                    y += 0;
                }
              
                output[batch][output_feature] = y;
             
            }
            auto b = bias[output_feature];
      
            output[batch][output_feature] += b;
     
            
        }
    }
    
}

vector<vector<int32_t>> linear_kernel(
    vector<vector<int32_t>>& input,
    vector<vector<int8_t>>& shift,
    vector<vector<int32_t>>& sign ,
    vector<int32_t>& bias)
{
   
    vector<int32_t> n(shift.size(), 0); 
    vector<vector<int32_t>> output(input.size(), n);

    //****************************************
    // #pragma omp parallel num_threads(10)
    at::parallel_for(0, input.size(), 10, [&](int64_t start, int64_t end){
    //  for( unsigned int  batch = 0 ;  batch < input.size(); batch++){
        for( auto batch = start ;  batch < end; batch++){
        for(unsigned int output_feature = 0 ; output_feature < shift.size(); output_feature++){
            for(unsigned int input_feature = 0; input_feature <input[0].size();input_feature++){
                // cout<<"0"<<endl;
                auto s = shift[output_feature][input_feature];
                // cout<<"1"<<endl;
                auto y = output[batch][output_feature];
                // cout<<"2"<<endl;
                auto x = input[batch][input_feature];
                // cout<<"3"<<endl;
                if(sign[output_feature][input_feature] < 0 && s >=0 ){
                    y -= (x << s);
                }
                else if(sign[output_feature][input_feature] > 0 && s >=0){
                    y += (x << s);
                }
                else if(sign[output_feature][input_feature] < 0 && s <0){
                    y -= (x >> (-s));
                }
                else if(sign[output_feature][input_feature] > 0 && s <0) {
                    y += (x >> (-s));
                }
                output[batch][output_feature] = y;
   
            }
            auto b = bias[output_feature];
            // cout<<"5"<<endl;
            output[batch][output_feature] += b;
            // cout<<"6"<<endl;
        }
        
     }
    });

    return output;
}   

vector<vector<vector<vector<int32_t>>>> convolution_kernel(
    vector<vector<vector<vector<int32_t>>>>& input,
    vector<vector<vector<vector<int8_t>>>>& shift,
    vector<vector<vector<vector<int32_t>>>>& sign ,
    vector<int32_t>& bias,
    torch::IntArrayRef strides,
    torch::IntArrayRef padding)
{
    // std::clock_t start;
    // double duration;
    // start = std::clock();
    int strides_h;
    int strides_w;
    if(strides.size() ==1){
        strides_h = strides[0];
        strides_w = strides[0];
    }
    else{
        strides_h = strides[0];
        strides_w = strides[1];
    }
    // torch::Tensor input = torch::constant_pad_nd(input_,padding, 0);
    int out_height = (input[0][0].size() - shift[0][0].size()) / strides_h +1;
    
    int out_width = (input[0][0][0].size() - shift[0][0][0].size()) / strides_w +1;
    
    // torch::Tensor output = torch::zeros({input.size(),shift.size(),out_height, out_width }, torch::dtype(torch::kInt32));
    vector<int32_t> l1(out_width,0); 
    vector<vector<int32_t>> l2(out_height,l1);
    vector<vector<vector<int32_t>>> l3(shift.size(), l2);
    vector<vector<vector<vector<int32_t>>>> output(input.size(), l3);
    at::parallel_for(0, output.size(), 0, [&](int64_t start, int64_t end){
    for (unsigned int batch = start; batch < end; batch++) {//batch
      for (unsigned int out_channel = 0; out_channel < output[0].size(); out_channel++) {//out_channel
            auto b = bias[out_channel];
         for (unsigned int out_height = 0; out_height < output[0][0].size(); out_height++) {//out_height
            for (unsigned int out_width = 0; out_width < output[0][0][0].size(); out_width++) {//out_width
               for (unsigned int filter_height = 0; filter_height < shift[0][0].size(); filter_height++) {//filter_height
                  for (unsigned int filter_width = 0; filter_width < shift[0][0][0].size(); filter_width++) {//filter_width
                     for (unsigned int in_channel = 0; in_channel < input[0].size(); in_channel++) {//in_channel
                                
                                auto s = shift[out_channel][in_channel][filter_height][filter_width];
                              
                                auto y = output[batch][out_channel][out_height][out_width];
                              
                                auto x = input[batch][in_channel][out_height * strides_h + filter_height][out_width * strides_w + filter_width];
                               
                                if(sign[out_channel][in_channel][filter_height][filter_width] < 0 && s >=0 ){
                                    y -= (x << s);
                                }
                                else if(sign[out_channel][in_channel][filter_height][filter_width] > 0 && s >=0){
                                    y += (x << s);
                                }
                                else if(sign[out_channel][in_channel][filter_height][filter_width] < 0 && s <0){
                                    y -= (x >> (-s));
                                }
                                else{
                                    y += (x >> (-s));
                                }
                        output[batch][out_channel][out_height][out_width] =y;

                     }
                  }
               }
                 
                 output[batch][out_channel][out_height][out_width] +=b;
            }
         }
            
            
      }
   }
    });
    return output;
}

# python对数量化
def round_to_fixed(input, integer_bits=16, fraction_bits=16): 
    assert integer_bits >= 1, integer_bits
    # TODO: Deal with unsigned tensors where there is no sign bit
    #       which is the case with activations to convolution that 
    #       are usually the output of a Relu layer
    if integer_bits == 1: 
        return torch.sign(input) - 1 
    delta = math.pow(2.0, -(fraction_bits))
    bound = math.pow(2.0, integer_bits-1) 
    min_val = - bound 
    max_val = bound - 1 
    rounded = torch.floor(input / delta) * delta

    clipped_value = torch.clamp(rounded, min_val, max_val)
    return clipped_value 

def get_shift_and_sign(x, rounding='deterministic'):
    sign = torch.sign(x)
    
    x_abs = torch.abs(x)
    shift = round(torch.log(x_abs) / np.log(2), rounding)
    
    return shift, sign    

def round_power_of_2(x, rounding='deterministic'):
    shift, sign = get_shift_and_sign(x, rounding)    
    x_rounded = (2.0 ** shift) * sign
    return x_rounded

def round(x, rounding='deterministic'):
    assert(rounding in ['deterministic', 'stochastic'])
    if rounding == 'stochastic':
        x_floor = x.floor()
        return x_floor + torch.bernoulli(x - x_floor)
    else:
        return x.round()

def clampabs(input, min, max):
    assert(min >= 0 and max >=0)

    input[input > max] = max
    input[input < -max] = -max

    input[(input > torch.zeros_like(input)) & (input < min)] = min
    input[(input < torch.zeros_like(input)) & (input > -min)] = -min
    return input 

#concatenate shift and sign together
def compress_bits(shift, sign):
    conc_weight = ConcWeight() 

    if len(shift.shape) == 2:
        shift = shift.unsqueeze(-1).unsqueeze(-1)

    # if sign is ternary, then use a big shift value that is equivalent to multiplying by zero
    zero_sign_indices = (sign == 0).nonzero()
    shift[zero_sign_indices] = -32
    sign[zero_sign_indices] = +1

    conc_weight.bits = math.ceil(torch.log( - torch.min(shift) + 1)/ np.log(2))
    # treat shift to the right as the default
    shift = shift * -1
    minimum = int(torch.min(shift))
    if minimum < 0:
        conc_weight.base = minimum
        shift = shift - minimum
    else:
        conc_weight.base = 0

    num = int(32 / (conc_weight.bits + 1))
    row_length = int((shift.shape[1] * shift.shape[2] * shift.shape[3] + num -1) / num )
    size = row_length * shift.shape[0]

    conc_weight.data = compress_sign_and_shift(shift.int().cuda(), sign.int().cuda(), size, conc_weight.base, conc_weight.bits, row_length, num)

    return conc_weight
    
    log2 = math.log(2)

# 对数量化优化器
class SGDShift(torch.optim.SGD):
    def step(self, closure=None):
        """Performs a single optimization step.
        Arguments:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        """
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            weight_decay = group['weight_decay']
            momentum = group['momentum']
            dampening = group['dampening']
            nesterov = group['nesterov']

            for p in group['params']:
                if p.grad is None:
                    continue
                d_p = p.grad.data
                if weight_decay != 0:
                    d_p.add_(weight_decay, 2**(p.data) * log2)
                if momentum != 0:
                    param_state = self.state[p]
                    if 'momentum_buffer' not in param_state:
                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()
                    else:
                        buf = param_state['momentum_buffer']
                        buf.mul_(momentum).add_(1 - dampening, d_p)
                    if nesterov:
                        d_p = d_p.add(momentum, buf)
                    else:
                        d_p = buf

                p.data.add_(-group['lr'], d_p)

        return loss

// C++ 稀疏矩阵运算和压缩
namespace SparseMatrix {
    class Exception : public std::exception {
    public:
        explicit Exception(const std::string &message) : exception(), message(message) {}

        virtual ~Exception(void) throw() {}

        inline std::string getMessage(void) const {
            return this->message;
        }

    protected:
        std::string message;
    };

    class InvalidDimensionsException : public Exception {
    public:
        InvalidDimensionsException(const std::string &message) : Exception(message) {}

    };

    class InvalidCoordinatesException : public Exception {
    public:
        InvalidCoordinatesException(const std::string &message) : Exception(message) {}
    };

}

#include <bitset>

namespace SparseMatrix {

    template<typename T>
    class SparseMatrix {
    public:
        SparseMatrix(int n);

        SparseMatrix(int rows, int columns);

        SparseMatrix(const SparseMatrix<T> &m);

        SparseMatrix<T> &operator=(const SparseMatrix<T> &m);

        ~SparseMatrix(void);

        int getRowCount(void) const;

        int getColumnCount(void) const;

        T get(int row, int col) const;

        SparseMatrix &set(T val, int row, int col);

        std::vector<T> multiply(const std::vector<T> &x) const;

        std::vector<T> operator*(const std::vector<T> &x) const;

        SparseMatrix<T> multiply(const SparseMatrix<T> &m) const;

        SparseMatrix<T> operator*(const SparseMatrix<T> &m) const;

        SparseMatrix<T> add(const SparseMatrix<T> &m) const;

        SparseMatrix<T> operator+(const SparseMatrix<T> &m) const;

        SparseMatrix<T> subtract(const SparseMatrix<T> &m) const;

        SparseMatrix<T> operator-(const SparseMatrix<T> &m) const;

        template<typename X>
        friend bool operator==(const SparseMatrix<X> &a, const SparseMatrix<X> &b);

        template<typename X>
        friend bool operator!=(const SparseMatrix<X> &a, const SparseMatrix<X> &b);

        template<typename X>
        friend std::ostream &operator<<(std::ostream &os, const SparseMatrix<X> &matrix);


    protected:
        int m, n;
        std::vector<T> *vals;
        std::vector<int> *rows, *cols;

        void construct(int m, int n);

        void destruct(void);

        void deepCopy(const SparseMatrix<T> &m);

        void validateCoordinates(int row, int col) const;

        void insert(int index, int row, int col, T val);

        void remove(int index, int row);

    public:
        class Bitmap;

        Bitmap &BitMapCompression();

        void CSRCompression(vector<int> &values, vector<int> &cols, vector<int> &rows);
    };

    constexpr static int N = 1000;

    class Bitmap {
    public:
        Bitmap() = default;

        Bitmap(int cols_, int rows_) : cols(cols_), rows(rows_) {}

    public:
        int cols, rows;
        std::bitset<N> bitmaps[N];
    };

    template<typename T>
    SparseMatrix<T>::SparseMatrix(int n) {
        this->construct(n, n);
    }

    template<typename T>
    SparseMatrix<T>::SparseMatrix(int rows, int columns) {
        this->construct(rows, columns);
    }

    template<typename T>
    SparseMatrix<T>::SparseMatrix(const SparseMatrix<T> &matrix) {
        this->deepCopy(matrix);
    }

    template<typename T>
    SparseMatrix<T> &SparseMatrix<T>::operator=(const SparseMatrix<T> &matrix) {
        if (&matrix != this) {
            this->destruct();
            this->deepCopy(matrix);
        }

        return *this;
    }

    template<typename T>
    void SparseMatrix<T>::deepCopy(const SparseMatrix<T> &matrix) {
        this->m = matrix.m;
        this->n = matrix.n;
        this->rows = new std::vector<int>(*(matrix.rows));

        if (matrix.vals != NULL) {
            this->cols = new std::vector<int>(*(matrix.cols));
            this->vals = new std::vector<T>(*(matrix.vals));
        }
    }

    template<typename T>
    SparseMatrix<T>::~SparseMatrix(void) {
        this->destruct();
    }

    template<typename T>
    void SparseMatrix<T>::construct(int rows, int columns) {
        if (rows < 1 || columns < 1) {
            throw InvalidDimensionsException("Matrix dimensions cannot be zero or negative.");
        }

        this->m = rows;
        this->n = columns;

        this->vals = NULL;
        this->cols = NULL;
        this->rows = new std::vector<int>(rows + 1, 1);
    }

    template<typename T>
    void SparseMatrix<T>::destruct(void) {
        if (this->vals != NULL) {
            delete this->vals;
            delete this->cols;
        }

        delete this->rows;
    }

    template<typename T>
    int SparseMatrix<T>::getRowCount(void) const {
        return this->m;
    }

    template<typename T>
    int SparseMatrix<T>::getColumnCount(void) const {
        return this->n;
    }

    template<typename T>
    T SparseMatrix<T>::get(int row, int col) const {
        this->validateCoordinates(row, col);

        int currCol;

        for (int pos = (*(this->rows))[row - 1] - 1; pos < (*(this->rows))[row] - 1; ++pos) {
            currCol = (*(this->cols))[pos];

            if (currCol == col) {
                return (*(this->vals))[pos];

            } else if (currCol > col) {
                break;
            }
        }

        return T();
    }

    template<typename T>
    SparseMatrix<T> &SparseMatrix<T>::set(T val, int row, int col) {
        this->validateCoordinates(row, col);

        int pos = (*(this->rows))[row - 1] - 1;
        int currCol = 0;

        for (; pos < (*(this->rows))[row] - 1; pos++) {
            currCol = (*(this->cols))[pos];

            if (currCol >= col) {
                break;
            }
        }

        if (currCol != col) {
            if (!(val == T())) {
                this->insert(pos, row, col, val);
            }

        } else if (val == T()) {
            this->remove(pos, row);

        } else {
            (*(this->vals))[pos] = val;
        }

        return *this;
    }

    template<typename T>
    std::vector<T> SparseMatrix<T>::multiply(const std::vector<T> &x) const {
        if (this->n != (int) x.size()) {
            throw InvalidDimensionsException("Cannot multiply: Matrix column count and vector size don't match.");
        }

        std::vector<T> result(this->m, T());

        if (this->vals != NULL) { // only if any value set
            for (int i = 0; i < this->m; i++) {
                T sum = T();
                for (int j = (*(this->rows))[i]; j < (*(this->rows))[i + 1]; j++) {
                    sum = sum + (*(this->vals))[j - 1] * x[(*(this->cols))[j - 1] - 1];
                }

                result[i] = sum;
            }
        }

        return result;
    }

    template<typename T>
    std::vector<T> SparseMatrix<T>::operator*(const std::vector<T> &x) const {
        return this->multiply(x);
    }

    template<typename T>
    SparseMatrix<T> SparseMatrix<T>::multiply(const SparseMatrix<T> &m) const {
        if (this->n != m.m) {
            throw InvalidDimensionsException(
                    "Cannot multiply: Left matrix column count and right matrix row count don't match.");
        }

        SparseMatrix<T> result(this->m, m.n);

        T a;

        for (int i = 1; i <= this->m; i++) {
            for (int j = 1; j <= m.n; j++) {
                a = T();

                for (int k = 1; k <= this->n; k++) {
                    a = a + this->get(i, k) * m.get(k, j);
                }

                result.set(a, i, j);
            }
        }
        return result;
    }

    template<typename T>
    SparseMatrix<T> SparseMatrix<T>::operator*(const SparseMatrix<T> &m) const {
        return this->multiply(m);
    }

    template<typename T>
    SparseMatrix<T> SparseMatrix<T>::add(const SparseMatrix<T> &m) const {
        if (this->m != m.m || this->n != m.n) {
            throw InvalidDimensionsException("Cannot add: matrices dimensions don't match.");
        }

        SparseMatrix<T> result(this->m, this->n);

        for (int i = 1; i <= this->m; i++) {
            for (int j = 1; j <= this->n; j++) {
                result.set(this->get(i, j) + m.get(i, j), i, j);
            }
        }

        return result;
    }

    template<typename T>
    SparseMatrix<T> SparseMatrix<T>::operator+(const SparseMatrix<T> &m) const {
        return this->add(m);
    }

    template<typename T>
    SparseMatrix<T> SparseMatrix<T>::subtract(const SparseMatrix<T> &m) const {
        if (this->m != m.m || this->n != m.n) {
            throw InvalidDimensionsException("Cannot subtract: matrices dimensions don't match.");
        }

        SparseMatrix<T> result(this->m, this->n);

        for (int i = 1; i <= this->m; i++) {
            for (int j = 1; j <= this->n; j++) {
                result.set(this->get(i, j) - m.get(i, j), i, j);
            }
        }

        return result;
    }

    template<typename T>
    SparseMatrix<T> SparseMatrix<T>::operator-(const SparseMatrix<T> &m) const {
        return this->subtract(m);
    }

    template<typename T>
    void SparseMatrix<T>::validateCoordinates(int row, int col) const {
        if (row < 1 || col < 1 || row > this->m || col > this->n) {
            throw InvalidCoordinatesException("Coordinates out of range.");
        }
    }

    template<typename T>
    void SparseMatrix<T>::insert(int index, int row, int col, T val) {
        if (this->vals == NULL) {
            this->vals = new std::vector<T>(1, val);
            this->cols = new std::vector<int>(1, col);

        } else {
            this->vals->insert(this->vals->begin() + index, val);
            this->cols->insert(this->cols->begin() + index, col);
        }

        for (int i = row; i <= this->m; i++) {
            (*(this->rows))[i] += 1;
        }
    }

    template<typename T>
    void SparseMatrix<T>::remove(int index, int row) {
        this->vals->erase(this->vals->begin() + index);
        this->cols->erase(this->cols->begin() + index);

        for (int i = row; i <= this->m; i++) {
            (*(this->rows))[i] -= 1;
        }
    }

    template<typename T>
    void SparseMatrix<T>::CSRCompression(vector <T> &values, vector<int> &cols, vector<int> &rows) {
        values.assign(this->vals->begin(), this->vals->end());
        cols.assign(this->cols->begin(), this->cols->end());
        rows.assign(this->begin(), this->end());
    }

    template<typename T>
    Bitmap &SparseMatrix<T>::BitMapCompression() {
        int m = this->m, n = this->n;
        auto bits = Bitmap(m, n);
        bis.bitmap.reset();
        for (int row_idx = 0; row_idx < m; ++row_idx) {
            for (int col_idx = 0; col_idx < n; ++col_idx) {
                if (!static_cast<int>(this->get(row_idx, col_idx))) {
                    bits.bitmaps[row_idx].set(col_idx);
                }
            }
        }
        return bits;
    }

    template<typename T>
    bool operator==(const SparseMatrix<T> &a, const SparseMatrix<T> &b) {
        return ((a.vals == NULL && b.vals == NULL)
                || (a.vals != NULL && b.vals != NULL && *(a.vals) == *(b.vals)))
               && ((a.cols == NULL && b.cols == NULL)
                   || (a.cols != NULL && b.cols != NULL && *(a.cols) == *(b.cols)))
               && *(a.rows) == *(b.rows);
    }

    template<typename T>
    bool operator!=(const SparseMatrix<T> &a, const SparseMatrix<T> &b) {
        return !(a == b);
    }

    template<typename T>
    std::ostream &operator<<(std::ostream &os, const SparseMatrix<T> &matrix) {
        for (int i = 1; i <= matrix.m; i++) {
            for (int j = 1; j <= matrix.n; j++) {
                if (j != 1) {
                    os << " ";
                }
                os << matrix.get(i, j);
            }
            if (i < matrix.m) {
                os << std::endl;
            }
        }
        return os;
    }
}

// 32位浮点数乘法
module float_mul(
clk,
rst_n,
float_a,
float_b,
float_o,
round,
overflow
);

input clk;
input rst_n;
input round;
input  signed [31:0] float_a;
input  signed [31:0] float_b;

output reg signed [31:0] float_o;
reg n;

output reg overflow;

reg s_a, s_b, s_o;//sign
reg [7:0] exp_a, exp_b;
reg [7:0] exp_o;//exponent
reg [22:0] sign_a, sign_b;
reg [22:0] sign_o;//significand
reg [23:0] temp;
reg [8:0] flow;
reg [1:0] int;
reg [23:0] dec;
reg [45:0] data;

always @(posedge clk or negedge rst_n)
begin
        if(!rst_n) 
                begin
                        s_a <= 0;
                        exp_a <= 0;
                        sign_a <= 0;
                
                        s_b <= 0;
                        exp_b <= 0;
                        sign_b <= 0;
                end
        else 
                begin
                        s_a <= float_a[31];
                        exp_a <= float_a[30:23];
                        sign_a <= float_a[22:0];
                        
                        s_b <= float_b[31];
                        exp_b <= float_b[30:23];
                        sign_b <= float_b[22:0];
                end                        
end 

always @(posedge clk)
begin
        if(sign_a == 23'b00000000_00000000_0000000)
                begin
                        sign_o <= sign_b;
                        n <= 0;
                end
        else if(sign_b == 23'b00000000_00000000_0000000)
                        begin
                                sign_o <= sign_a;
                                n <= 0;
                        end
        else 
                begin
                        temp <= sign_a + sign_b;
                        data <= sign_a * sign_b;//1.m*1.n=1+0.m+0.n+(0.m*0.n)
                        dec <= data[45:23] + temp[22:0];
                
                        int <= 1'b1 + temp[23];
                        int <= int + dec[23];
                        
                if(int[1] == 1) 
                        begin
                                n <= 1;
                                if(int[0] == 1) 
                                        begin
                                        if(!round) 
                                                begin
                                                        
                                                        sign_o = {1'b1, dec[22:0]};//零舍入
                                                end
                                        
                                        if(round) 
                                                begin
                                                        if(data[22] == 1) 
                                                                begin
                                                                        sign_o = {1'b1, dec[22:0]} + 1;//进1
                                                                end
                                         else sign_o = {1'b1, dec[22:0]};
                                        end
                        end
                else 
                        begin
                                
                                if(!round) 
                                        begin
                                        
                                                sign_o = {1'b0, dec[22:1]};//零舍入
                                        end
                                
                                if(round) 
                                        begin
                                        if(data[22] == 1) 
                                                begin
                                                        sign_o = {1'b0, dec[22:1]} + 1;//进1
                                                end
                                         else sign_o = {1'b0, dec[22:1]};
                                                end
                                        end
                        end                        
                else 
                        begin
                          n <= 0;
                          sign_o <= dec[22:0];                        
                        end
                end
end                        
                        
always @(posedge clk or negedge rst_n)
begin
        if(!rst_n) 
                begin
                        exp_o <= 0;
                        flow <= 0;
                end
        else
                begin
                        flow <= (exp_a - 8'd127) + (exp_b - 8'd127) + 8'd127 + n;
                        exp_o <= flow[7:0];
                end
end

always @(posedge clk or negedge rst_n)
begin
        if(!rst_n) 
                begin
                        float_o <= 0;
                        overflow <= 0;
                end
        else 
                begin
                        s_o = s_a^s_b;
                        if(flow[8] == 1'b1) begin
                                        overflow <= 1;
                                        float_o <= 32'b0;        
                        end
                        else        float_o <= {s_o, exp_o[7:0], sign_o[22:0]};

                end
end

endmodule

def sigmoid_pwl(x):
    out = np.zeros_like(x)

    con1 = np.abs(x) >= 5
    out[con1] = 1
    con2 = np.logical_and(np.abs(x) < 5, np.abs(x) >= 2.375)
    out[con2] = 0.03125 * np.abs(x[con2]) + 0.84375
    con3 = np.logical_and(np.abs(x) < 2.375, np.abs(x) >= 1)
    out[con3] = 0.125 * np.abs(x[con3]) + 0.625
    con4 = np.logical_and(np.abs(x) < 1, np.abs(x) >= 0)
    out[con4] = 0.25 * np.abs(x[con4]) + 0.5
    con5 = x < 0
    out[con5] = 1 - out[con5]
    return out